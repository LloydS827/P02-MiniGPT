# LLM项目


## Project Structure

```
.
├── src/                    # Source code
│   ├── data/              # Data processing modules
│   ├── models/            # Model definition modules
│   ├── training/          # Training logic modules
│   ├── evaluation/        # Evaluation logic modules
│   └── utils/             # Utility functions
├── data/                  # Data directory
│   ├── raw/              # Raw data
│   └── processed/        # Processed data
├── configs/               # Configuration files
├── models/                # Trained model checkpoints
├── notebooks/            # Jupyter notebooks for experiments
└── docs/                 # Documentation and reports
```

## Resources

### LLMFS
- [LLMFS](https://github.com/rasbt/LLMs-from-scratch)
  - [Build an LLM from Scratch 7: Instruction Finetuning - YouTube](https://www.youtube.com/watch?v=4yNswvhPWCQ&list=PLTKMiZHVd_2IIEsoJrWACkIxLRdfMlw11&index=1)

### Karpathy
- [Let's build the GPT Tokenizer - YouTube](https://www.youtube.com/watch?v=zduSFxRajkE&t=9s)
- [Let's reproduce GPT-2 (124M)](https://www.youtube.com/watch?v=l8pRSuU81PU&t=2s)

### Others
- [Understand and Code DeepSeek V3](https://www.freecodecamp.org/news/understand-and-code-deepseek-v3/)
- [Attention in Transformers: Concepts and Code in PyTorch - DeepLearning.AI](https://www.deeplearning.ai/short-courses/attention-in-transformers-concepts-and-code-in-pytorch/)
- [How Transformer LLMs Work - DeepLearning.AI](https://www.deeplearning.ai/short-courses/how-transformer-llms-work/)
